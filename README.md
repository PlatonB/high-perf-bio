# Синопсис.
## Преимущества.
- Высокая скорость работы:
    - вплоть до 8 процессов одновременно;
    - в основе - известная своей производительностью СУБД _MongoDB_.
- Простота запуска любого компонента:
    - одна команда - обработка всех коллекций базы;
    - не более двух обязательных аргументов.
- Наглядность:
    - вывод примерной структуры интересующей БД.
- Полностью автоматическое чтение и создание VCF и BED.
- Всегда бесплатная [техподдержка](https://github.com/PlatonB/high-perf-bio/issues).

# Перед началом работы.
## Если кратко.
1. Разрешите [зависимости](https://github.com/PlatonB/high-perf-bio#установка-сторонних-компонентов).
2. Скачайте архив с инструментарием:
```
Clone or download
```
(зелёная кнопка наверху страницы репозитория).

3. Распакуйте его в любую папку.
4. Освойте [запуск программ из терминала](https://github.com/PlatonB/ngs-pipelines#преодолеваем-страх-командной-строки-linux).

## Установка сторонних компонентов.
### MongoDB.
Советую вначале ознакомиться с [основами работы в линуксовым терминале](https://github.com/PlatonB/ngs-pipelines#преодолеваем-страх-командной-строки-linux). Впрочем, если совсем лень, можете просто копировать, вставлять и запускать приведённые ниже команды. После установки настоятельно рекомендую перезагрузиться.

#### Ubuntu Linux.
([elementary OS](https://elementary.io/ru/)/KDE neon/Linux Mint)

Подключение официального репозитория _MongoDB_.
```
wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add -
```
```
echo "deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list
```

Обновление индекса пакетов ОС.
```
sudo apt update
```

Собственно, установка _MongoDB_.
```
sudo apt install -y mongodb-org
```

Перманентный запуск _MongoDB_. Лучше так сделать, если планируете использовать _high-perf-bio_ и [_ld-tools_](https://github.com/PlatonB/ld-tools) часто.
```
systemctl enable mongod.service
```

Если вам не нужно эксплуатировать _MongoDB_-решения каждый день, то рекомендую команду, активирующую _MongoDB_ до ближайшей перезагрузки.
```
sudo service mongod start
```

#### Fedora Linux.
TBD.

### PyMongo.
Установка с помощью _pip_:
```
pip3 install pymongo
```

Установка с помощью [_Conda_](https://github.com/PlatonB/ngs-pipelines#установка-conda):
```
conda install pymongo
```

### Примечание по поводу Windows.
Теоретически, после установки _MongoDB_ и whl-пакета _PyMongo_, программа должна работать. Но у меня сейчас _Windows_ нет, и я пока не проверял. Надеюсь, кто-нибудь поделится опытом в [Issues](https://github.com/PlatonB/high-perf-bio/issues).

# Тьюториал.
Компоненты _high-perf-bio_ управляются исключительно командами. Если вы имели дело только с графическими интерфейсами, можете почитать [мой материал о командной строке](https://github.com/PlatonB/ngs-pipelines#преодолеваем-страх-командной-строки-linux). Ничего сложного там нет, а если что, [обращайтесь](https://github.com/PlatonB/high-perf-bio/issues), я помогу.

Чтобы не прописывать каждый раз путь к тому или иному компоненту, перейдём в основную папку инструментария.
```
cd $HOME/Биоинформатика/high-perf-bio-master
```
Примечание: корректируйте приведённые в примерах пути в зависимости от реального расположения ваших папок или файлов.

Для удобства можете вывести имена всех программ проекта _high-perf-bio_.
```
ls
```

Каждый инструмент содержит подробную справку. Пока с ней не ознакомились, реальные задачи не запускайте.
```
python3 create_db.py -h
```
```
python3 annotator.py -h
```
и т.д..

В папке high-perf-bio-master уже есть небольшие примеры данных. Но в качестве материала для базы лучше скачать что-то серьёзное, например, VCF-таблицу [распространённых SNP](ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/common_all_20180418.vcf.gz).

Поскольку формат этого файла - VCF, программа создания БД сама отбросит технические строки, сконвертирует ячейки определённых столбцов в BSON-структуры и проиндексирует поля с хромосомами, позициями и rsIDs. Файл - один, значит, распараллелить загрузку не удастся. Таким образом, команда может получиться весьма минималистичной.
```
python3 create_db.py -S $HOME/Биоинформатика/dbSNP_common
```

Есть вероятность возникновения двух проблем: вы забыли, как называется БД и из чего она состоит. В таких ситуациях перед запуском любого парсера требуется получить представление о парсимой БД. Для начала найдите её имя в списке всех имён.
```
python3 print_db_info.py
```

Затем выведите имена коллекций, индексов и другие первостепенные характеристики конкретной базы.
```
python3 print_db_info.py -d dbSNP_common
```

Теперь мы многое знаем о БД и можем, к примеру, осуществить по ней запрос. Допустим, отберём снипы интервала chr14:105746998-105803861, упомянутого в статье "Локусы влияющие на экспрессию антигенов HLA в участке 14-й хромосомы, ассоциированном с развитием рассеянного склероза, и функции расположенных в них генов" (П.А. Быкадоров и др.).
```
python3 make_request.py -D dbSNP_common -T $HOME/Биоинформатика/результаты -q '{"#CHROM": "14", "POS": {"$gt": 105746998, "$lt": 105803861}}'
```

Давайте найдём характеристики SNPs папки high-perf-bio-master/test_data/TSV. С расположением rsID-столбца повезло - он первый. С базой dbSNP_common тоже всё ок - есть поле ID. Значит, обойдёмся минимумом аргументов: путь к игрушечным таблицам, имя базы и исключение табличной шапки. Результаты в этом примере попадут в исходную папку.
```
python3 annotator.py -S $HOME/Биоинформатика/high-perf-bio-master/test_data/TSV -D dbSNP_common -m 1
```

Интересно, а есть на свете снипы, являющиеся eQTL сразу во всех тканях? Сейчас посмотрим. Скачаем [таблицы значимых пар eQTL-eGene](https://storage.googleapis.com/gtex_analysis_v8/single_tissue_qtl_data/GTEx_Analysis_v8_eQTL.tar). Нам понадобятся только \*.egenes.txt.gz-файлы. \*.signif_variant_gene_pairs.txt.gz-файлы можете быстро удалить каким-нибудь продвинутым файлменеджером (типа _Double Commander_) или этим незамысловатым Python-скриптом:
```
import os
gtex_dir_path = 'путь_к_папке/GTEx_Analysis_v8_eQTL'
for gtex_file_name in os.listdir(gtex_dir_path):
        if gtex_file_name.find('signif') != -1:
                os.remove(os.path.join(gtex_dir_path, gtex_file_name))
```

Зальём оставшиеся таблицы в базу. Дадим базе легко воспринимаемое имя, проиндексируем коллекции по rsID-столбцу. Коллекций - несколько десятков, поэтому лично я предпочёл бы распараллелить размещение и индексацию на 8 процессов.
```
python3 create_db.py -S $HOME/Биоинформатика/GTEx_Analysis_v8_eQTL -d GTEx -i rs_id_dbSNP151_GRCh38p7 -p 8
```

Что такое левые/правые коллекции и глубина, подробно рассказано в справке _intersect_subtract_ - здесь я на этом останавливаться не буду. Указываем в явном виде одну левую коллекцию. Любую. Правыми коллекциями для решения задачи надо сделать все остальные, но перечислять их не нужно - для программы такой сценарий предусмотрен по дефолту. То, что нам нужно пересекать, а не вычитать, опять же, явно прописывать не обязательно. А вот имя пересекаемого поля программе от нас потребуется, ведь коллекции делались по таблицам небиоинформатического формата, а в этом случае применимы не все удобные умолчания. eQTL нам общие для всех тканей надо найти? Значит, пишем глубину, равную количеству всех коллекций минус 1 (вычли одну левую).
```
python3 intersect_subtract.py -D GTEx -T $HOME/Биоинформатика/результаты -l Adipose_Subcutaneous.v8.egenes.txt -f rs_id_dbSNP151_GRCh38p7 -d 48
```

Вездесущие eQTL обнаружились, и их 63 штуки. А сколько будет SNPs, влияющих на экспрессию генов только в крови и больше нигде? Это решается вычитанием.
```
python3 intersect_subtract.py -D GTEx -T $HOME/Биоинформатика/результаты -l Whole_Blood.v8.egenes.txt -f rs_id_dbSNP151_GRCh38p7 -a subtract -d 48
```

Таких уникальных снипов аж около половины - 10963 из 20315 всех кровяных.

Мы сейчас работали с полными наборами пар снипов-генов. Стоит заметить, что не во всех из них встречаются настоящие eGenes. Чтобы таковые получить, надо произвести отбор по порогу q-value<=0.05. Индекс rsID-поля для этого точно не потребуется, но заметный прирост скорости дал бы индекс поля с q-value. Узнаём имена удаляемого индекса и индексируемого поля, после чего готовим все коллекции к поиску eGenes.
```
python3 print_db_info.py -d GTEx
```
```
python3 reindex_db.py -D GTEx -r rs_id_dbSNP151_GRCh38p7_1 -a qval -p 8
```

Теперь фильтруем.
```
python3 make_request.py -D GTEx -T $HOME/Биоинформатика/результаты -q '{"qval": {"$lte": Decimal128("0.05")}}' -p 8
```

Полученные результаты можно отправить в новую базу, с чем вы уже точно справитесь самостоятельно.

# Механизм пересечения и вычитания.

РАЗДЕЛ В РАЗРАБОТКЕ!

Этот раздел - попытка максимально доступно объяснить, как работает программа _intersect_subtract_.

## Терминология.
| Термин | Аналог в реляционных БД | Аналог в Python | Пример | Пояснение |
| ------ | ----------------------- | --------------- | ------ | --------- |
| Поле | Столбец | Ключ словаря | `'f1'` | Может присутствовать в одних документах и при этом отсутствовать в других |
| Документ | Строка | Словарь | `{'f1': 1, 'f2': 'one', 'f3': 'c1.txt'}` | Объект из пар поле-значение. В качестве значений могут быть вложенные объекты |
| Коллекция | Таблица | Список словарей | | Набор, как правило, объединённых по смыслу документов |

## Тестовые данные.
В качестве примера будет БД из трёх игрушечных коллекций, которые легко пересекать и вычитать в уме. \_id-поле здесь и далее опускается.

Коллекция c1.
```
{'f1': 1, 'f2': 'one', 'f3': 'c1.txt'}
```
```
{'f1': 2, 'f2': 'two', 'f3': 'c1.txt'}
```
```
{'f1': 3, 'f2': 'three', 'f3': 'c1.txt'}
```
```
{'f1': 4, 'f2': 'four', 'f3': 'c1.txt'}
```
```
{'f1': 5, 'f2': 'five', 'f3': 'c1.txt'}
```

Коллекция c2.
```
{'f1': 3, 'f2': 'three', 'f3': 'c2.txt'}
```
```
{'f1': 4, 'f2': 'four', 'f3': 'c2.txt'}
```
```
{'f1': 5, 'f2': 'five', 'f3': 'c2.txt'}
```
```
{'f1': 6, 'f2': 'six', 'f3': 'c2.txt'}
```
```
{'f1': 7, 'f2': 'seven', 'f3': 'c2.txt'}
```

Коллекция c3.
```
{'f1': 5, 'f2': 'five', 'f3': 'c3.txt'}
```
```
{'f1': 6, 'f2': 'six', 'f3': 'c3.txt'}
```
```
{'f1': 7, 'f2': 'seven', 'f3': 'c3.txt'}
```
```
{'f1': 8, 'f2': 'eight', 'f3': 'c3.txt'}
```
```
{'f1': 9, 'f2': 'nine', 'f3': 'c3.txt'}
```

## Параметры отбора документов левой коллекции.
По справке к _intersect_subtract_ вы уже знаете, что одна из коллекций, условно обозначенных как "левые", фильтруется по всем "правым". Левым пусть будет c1, в в качестве правых - c2 и c3. Общее представление об условях отбора документов c1:
| Действие | Глубина | Документ сохраняется, если |
| -------- | ------- | ---------------------------- |
| пересечение | 1 | совпадение в c2 или c3 |
| пересечение | 2 | совпадение в c2 и c3 |
| вычитание | 1 | несовпадение в c2 или c3 |
| вычитание | 2 | несовпадение в c2 и c3 |

## Решение.
1. **Левостороннее внешнее объединение**. Независимо от того, хотим мы пересекать или вычитать, выполнение `$lookup`-инструкции лишь объединит коллекции. Каждый документ, получающийся в результате объединения, содержит: 1. все поля документа левой коллекции; 2. поля с соответствиями из правых коллекций (далее - результирующие). Если для элемента выбранного поля данного левого документа не нашлось совпадений в одной из правых коллекций, то в результирующем поле появится пустой список (Python-представление Null-значения из мира баз данных). Если же совпадение имеется, то в качестве значения результирующего поля возвратится список с содержимым соответствующего правого документа. Если выявилось совпадение с несколькими документами какой-то одной правой коллекции, то они в полном составе поступят в результирующее поле.

Схема объёдинённого документа.
```
{левый документ, псевдоним правой коллекции 1: [правый документ 1, правый документ 2, ...],
                 псевдоним правой коллекции 2: [правый документ 1, правый документ 2, ...],
                 ...}
```

Объединённые документы тестовых коллекций.
```
{'f1': 1, 'f2': 'one', 'f3': 'c1.txt', 'c2': [], 'c3': []}
```
```
{'f1': 2, 'f2': 'two', 'f3': 'c1.txt', 'c2': [], 'c3': []}
```
```
{'f1': 3, 'f2': 'three', 'f3': 'c1.txt', 'c2': [{'f1': 3, 'f2': 'three', 'f3': 'c2.txt'}], 'c3': []}
```
```
{'f1': 4, 'f2': 'four', 'f3': 'c1.txt', 'c2': [{'f1': 4, 'f2': 'four', 'f3': 'c2.txt'}], 'c3': []}
```
```
{'f1': 5, 'f2': 'five', 'f3': 'c1.txt', 'c2': [{'f1': 5, 'f2': 'five', 'f3': 'c2.txt'}], 'c3': [{'f1': 5, 'f2': 'five', 'f3': 'c3.txt'}]}
```

2. **Фильтрация**. Изучая предыдущий пункт, вы уже, наверное, догадались, что фильтровать результаты пересечения или вычитания можно по количеству пустых или непустых результирующих списков. Теперь про повторы: в конечный файл направятся все дубли элемента, находящиеся в пределах поля левой коллекции, но от повторов правых элементов копийность результатов не зависит.

# Полезные советы.
## Метастроки.
У части тулзов _high_perf_bio_ есть опция --meta-lines-quan/-m. Она даёт программе знать, сколько строк в начале файла не нужно никак трогать. Эти строки содержат т.н. метаинформацию, которая посвящает исследователя в различные особенности файла. Программам же они, как правило, только мешают. VCF - формат довольно строгий, и там метастроки чётко обозначены символами ##, что даёт возможность скипать их автоматически. Для остальных форматов требуется ручное указание количества игнорируемых строк. Как это количество узнать? Если файл - маленький, просто откройте его в обычном блокноте. В хороших блокнотах, как, например, [elementary OS](https://elementary.io/ru/) Code, есть нумерация строк, что облегчает задачу. Если же большой, то так сделать не получится - в лучшем случае вылезет ошибка, в худшем - осложните себе работу зависанием. Вас раздражает командная строка? Можете считывать первые строки сжатого файла скриптом-предпросмотрщиком из моего репозитория [bioinformatic-python-scripts](https://github.com/PlatonB/bioinformatic-python-scripts). Уже привыкли к эмулятору терминала? Тогда юзайте командную утилиту _zless_.
```
zless -N $HOME/Биоинформатика/dbSNP_common/00-common_all.vcf.gz
```

Скроллить можно колесом мыши или двумя пальцами вверх-вниз по тачпаду. Закрыть предпросмотр: `q`.

## Спецсимволы.
В ту или иную команду могут закрасться зарезервированные символы. Командная оболочка, на них наткнувшись, не сможет обработать вашу команду как единое целое. Я, к примеру, при тестировании _high_perf_bio_ получил ошибку из-за решётки в начале имени хромосомного поля.
```
python3 create_db.py -S $HOME/Биоинформатика/dbSNP_common -i #CHROM,POS,ID
```
```
create_db.py: error: argument -i/--ind-col-names: expected one argument
```

Из-за наличия решётки интерпретатор подумал, что CHROM,POS,ID - комментарий, а не перечисление индексируемых полей. Получилось, что аргумент -i, строго требующий значения, остался без такового. Надо было просто взять набор имён полей в одинарные кавычки (заэкранировать).
```
python3 create_db.py -S $HOME/Биоинформатика/dbSNP_common -i '#CHROM,POS,ID'
```

А знаете, почему программы _high_perf_bio_ заставляют перечислять что-либо через запятую без привычного нам по естественным языкам пробела? Не подумайте, что из вредности:). Пробел считается границей между аргументами. Т.е., при использовании запятых с пробелом каждый следующий элемент воспринимался бы шеллом в качестве нового аргумента.
```
python3 create_db.py -S $HOME/Биоинформатика/high-perf-bio-master/test_data/TSV -i SYMBOL, AF
```
```
create_db.py: error: unrecognized arguments: AF
```

Требование перечислять без пробела я ввёл, чтобы избавить вас от необходимости экранирования. Правильное оформление последней команды не подразумевает обязательного наличия надоедливых кавычек.
```
python3 create_db.py -S $HOME/Биоинформатика/high-perf-bio-master/test_data/TSV -i SYMBOL,AF
```
